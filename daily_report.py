
import os
import smtplib
import datetime
import requests
import feedparser
import time
import calendar
import io
import json
import base64
import yfinance as yf
# Remove Matplotlib imports
# import matplotlib.pyplot as plt
# import matplotlib.dates as mdates
# from matplotlib.gridspec import GridSpec
import pandas as pd
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from jinja2 import Template
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import urllib3
import ssl
from openai import OpenAI

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š (é’ˆå¯¹æœ¬åœ° SSL é—®é¢˜)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# å…¨å±€ç¦ç”¨ SSL éªŒè¯ (è§£å†³ yfinance/curl 77 é”™è¯¯)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# è§£å†³ CURL è¯ä¹¦é—®é¢˜ (é’ˆå¯¹ Windows ä¸­æ–‡è·¯å¾„ + curl_cffi)
try:
    import curl_cffi.requests
    original_init = curl_cffi.requests.Session.__init__
    def new_init(self, *args, **kwargs):
        kwargs['verify'] = False
        original_init(self, *args, **kwargs)
    curl_cffi.requests.Session.__init__ = new_init
    print("Applied curl_cffi SSL patch")
except ImportError:
    pass

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv()

# --- é…ç½®éƒ¨åˆ† ---
MAIL_HOST = os.environ.get("MAIL_HOST", "smtp.qq.com")
MAIL_PORT = int(os.environ.get("MAIL_PORT", 465))
MAIL_USER = os.environ.get("MAIL_USERNAME", "")
MAIL_PASS = os.environ.get("MAIL_PASSWORD", "")
MAIL_RECEIVER = os.environ.get("MAIL_RECIPIENT", "")

# LLM é…ç½®
LLM_API_KEY = os.environ.get("LLM_API_KEY", "")
LLM_BASE_URL = os.environ.get("LLM_BASE_URL") or "https://api.deepseek.com"
LLM_MODEL = os.environ.get("LLM_MODEL") or "deepseek-chat"

# å†å²æ•°æ®æ–‡ä»¶
HISTORY_FILE = "indicator_history.json"

class IndicatorMonitor:
    def __init__(self):
        self.data_summary = {} 
        self.history = self.load_history()

    def load_history(self):
        """åŠ è½½æœ¬åœ°å†å²æ•°æ®"""
        if os.path.exists(HISTORY_FILE):
            try:
                with open(HISTORY_FILE, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Error loading history: {e}")
        return {}

    def save_history(self):
        """ä¿å­˜å†å²æ•°æ®"""
        try:
            with open(HISTORY_FILE, 'w') as f:
                json.dump(self.history, f, indent=2)
        except Exception as e:
            print(f"Error saving history: {e}")

    def update_history(self, key, value):
        """æ›´æ–°ç‰¹å®šæŒ‡æ ‡çš„å†å²æ•°æ® (æŒ‰æ—¥æœŸ)"""
        today = datetime.date.today().strftime("%Y-%m-%d")
        if key not in self.history:
            self.history[key] = {}
        self.history[key][today] = value
        self.save_history()

    def fetch_market_data(self):
        """è·å– DXY, US10Y, BTC, ETH, GOLD ç­‰æ•°æ®"""
        print("æ­£åœ¨è·å–å¸‚åœºæŒ‡æ ‡æ•°æ®...")
        tickers = {
            "DXY": "DX-Y.NYB",
            "US10Y": "^TNX",
            "BTC": "BTC-USD",
            "ETH": "ETH-USD",
            "GOLD": "GC=F"
        }
        
        try:
            data = yf.download(list(tickers.values()), period="1mo", interval="1d", progress=False)
            
            close_data = data['Close'] if isinstance(data.columns, pd.MultiIndex) else data
            
            summary = {}
            for name, ticker in tickers.items():
                if isinstance(data.columns, pd.MultiIndex):
                    if ticker not in close_data.columns: continue
                    series = close_data[ticker].dropna()
                else:
                    series = close_data.dropna()

                if not series.empty:
                    current = series.iloc[-1]
                    prev = series.iloc[-2] if len(series) > 1 else current
                    change = ((current - prev) / prev) * 100
                    summary[name] = {
                        "price": current,
                        "change": change
                    }
            
            self.data_summary.update(summary)
            return data
        except Exception as e:
            print(f"Error fetching market data: {e}")
            return None

    def fetch_farside_flow(self):
        """è·å– Farside Bitcoin ETF å‡€æµå…¥æ•°æ®"""
        print("æ­£åœ¨è·å– ETF æµå…¥æ•°æ® (Farside)...")
        url = "https://farside.co.uk/btc/"
        try:
            # impersonate chrome
            response = curl_cffi.requests.get(url, impersonate="chrome120", timeout=15, verify=False)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                tables = soup.find_all('table')
                target_table = None
                for table in tables:
                    text = table.get_text()
                    if "IBIT" in text and "FBTC" in text:
                        target_table = table
                        break
                
                if target_table:
                    rows = target_table.find_all('tr')
                    for row in reversed(rows):
                        cols = row.find_all(['td', 'th'])
                        data = [ele.get_text(strip=True) for ele in cols]
                        if not data: continue
                        if data[0] in ["Total", "Average", "Maximum", "Minimum"]: continue
                        
                        # Found valid row
                        date_str = data[0]
                        # Assuming last column is Total. 
                        # Based on test: ['', '', ..., 'Total'] mapped to values
                        # But sometimes headers are tricky. Usually last column is total.
                        total_flow = data[-1]
                        
                        self.data_summary['ETF_Flow'] = {
                            "date": date_str,
                            "value": total_flow
                        }
                        print(f"ETF Flow ({date_str}): {total_flow}M")
                        break
            else:
                print(f"Farside HTTP {response.status_code}")
        except Exception as e:
            print(f"Error fetching ETF data: {e}")

    def fetch_whale_count(self):
        """æŠ“å– >1000 BTC åœ°å€æ•°é‡"""
        print("æ­£åœ¨è·å–å·¨é²¸åœ°å€æ•°æ®...")
        url = "https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        try:
            r = requests.get(url, headers=headers, timeout=15, verify=False)
            soup = BeautifulSoup(r.text, 'html.parser')
            tables = soup.find_all('table')
            found = False
            for table in tables:
                if "Addresses" in table.get_text() and "1,000" in table.get_text():
                    rows = table.find_all('tr')
                    for row in rows:
                        cols = row.find_all('td')
                        if not cols: continue
                        label = cols[0].get_text(strip=True)
                        if any(x in label for x in ["1,000 - 10,000", "10,000 - 100,000", "100,000 - 1,000,000"]):
                            count_str = cols[1].get_text(strip=True).split(' ')[0].replace(',', '')
                            if count_str.isdigit():
                                current_val = int(count_str)
                                self.data_summary['Whales'] = current_val
                                self.update_history('whales', current_val)
                                found = True
                                
                                # Check 24h change logic if we have history
                                dates = sorted(self.history.get('whales', {}).keys())
                                if len(dates) >= 2:
                                    prev_date = dates[-2]
                                    prev_val = self.history['whales'][prev_date]
                                    diff = current_val - prev_val
                                    self.data_summary['Whales_Change_24h'] = diff
                                    print(f"å·¨é²¸åœ°å€æ•°: {current_val} (Change: {diff})")
                                else:
                                    print(f"å·¨é²¸åœ°å€æ•°: {current_val}")
                    break
        except Exception as e:
            print(f"Error fetching whale data: {e}")

    def fetch_fear_greed(self):
        """è·å–ææ…Œè´ªå©ªæŒ‡æ•°"""
        try:
            r = requests.get("https://api.alternative.me/fng/?limit=1", timeout=10)
            data = r.json()
            if data['data']:
                item = data['data'][0]
                self.data_summary['FearGreed'] = {
                    "value": int(item['value']),
                    "label": item['value_classification']
                }
        except Exception as e:
            print(f"Error fetching Fear & Greed: {e}")

    def fetch_derivatives_data(self):
        """è·å–èµ„é‡‘è´¹ç‡å’ŒæŒä»“é‡ (Source: Bybit)"""
        print("æ­£åœ¨è·å–è¡ç”Ÿå“æ•°æ® (Bybit)...")
        try:
            import curl_cffi.requests
            # Bybit V5 Market Ticker
            url = "https://api.bybit.com/v5/market/tickers?category=linear&symbol=BTCUSDT"
            
            r = curl_cffi.requests.get(
                url, 
                impersonate="chrome120", 
                timeout=10, 
                verify=False
            )
            
            if r.status_code == 200:
                data = r.json()
                if 'result' in data and 'list' in data['result'] and len(data['result']['list']) > 0:
                    item = data['result']['list'][0]
                    
                    # Funding Rate
                    fr_raw = float(item.get('fundingRate', 0))
                    fr_percent = fr_raw * 100
                    self.data_summary['FundingRate'] = fr_percent
                    
                    # Status
                    status = "ğŸŸ¢ å¥åº·"
                    if fr_percent > 0.03: status = "ğŸ”´ è¿‡çƒ­"
                    elif fr_percent < -0.005: status = "ğŸ”µ çœ‹ç©º"
                    self.data_summary['FundingRate_Status'] = status
                    print(f"èµ„é‡‘è´¹ç‡: {fr_percent:.4f}% ({status})")
                    
                    # Open Interest
                    oi_val = float(item.get('openInterestValue', 0))
                    oi_billion = oi_val / 1e9
                    self.data_summary['OpenInterest'] = oi_billion
                    print(f"æŒä»“é‡: ${oi_billion:.2f}B")
                else:
                    print("Bybit data format unexpected")
            else:
                print(f"Bybit API Error: {r.status_code}")
                
        except Exception as e:
            print(f"Error fetching Derivatives data: {e}")

    def fetch_defi_data(self):
        """è·å– DefiLlama ç¨³å®šå¸æ•°æ®"""
        print("æ­£åœ¨è·å–ç¨³å®šå¸å¸‚å€¼...")
        try:
            url = "https://stablecoins.llama.fi/stablecoins?includePrices=true"
            r = requests.get(url, timeout=10, verify=False)
            if r.status_code == 200:
                data = r.json()
                total_mcap = 0
                for coin in data['peggedAssets']:
                    if isinstance(coin, dict) and 'circulating' in coin:
                         total_mcap += coin['circulating'].get('peggedUSD', 0)
                
                # è½¬æ¢ä¸º Billion
                mcap_b = total_mcap / 1e9
                self.data_summary["Stablecoin_Mcap"] = mcap_b
                self.update_history('stablecoins', mcap_b)
                print(f"ç¨³å®šå¸å¸‚å€¼: ${mcap_b:.2f}B")
        except Exception as e:
            print(f"DefiLlama è·å–å¤±è´¥: {e}")

    def run(self):
        self.fetch_market_data()
        self.fetch_farside_flow()
        self.fetch_whale_count()
        self.fetch_fear_greed()
        self.fetch_derivatives_data()
        self.fetch_defi_data()
        return self.data_summary

def fetch_github_trending():
    """GitHub: topic:ai, stars:>500, weekly"""
    print("æ­£åœ¨è·å– GitHub çƒ­é—¨ AI é¡¹ç›®...")
    date_str = (datetime.date.today() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")
    # Query updated to topic:ai and stars:>500 as requested
    query = f"topic:ai stars:>500 created:>{date_str}"
    
    url = "https://api.github.com/search/repositories"
    params = {"q": query, "sort": "stars", "order": "desc", "per_page": 5}
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/vnd.github.v3+json"
    }
    github_token = os.environ.get("GITHUB_TOKEN")
    if github_token:
        headers["Authorization"] = f"Bearer {github_token}"

    try:
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        if resp.status_code == 200:
            items = resp.json().get("items", [])
            return [
                {
                    "name": item["full_name"],
                    "desc": item["description"] or "æš‚æ— æè¿°",
                    "stars": item["stargazers_count"],
                    "url": item["html_url"],
                    "language": item["language"] or "Unknown"
                }
                for item in items
            ]
    except Exception as e:
        print(f"GitHub è·å–å¤±è´¥: {e}")
    return []

def fetch_hacker_news_ai():
    """è·å– HN TopStories ä¸­å…³äº AI çš„è®¨è®º (Filtered by Score)"""
    print("æ­£åœ¨è·å– Hacker News AI è¯é¢˜ (Top/Best)...")
    try:
        session = requests.Session()
        # ä»…æ‰«æ TopStories å’Œ BestStoriesï¼Œç¡®ä¿çƒ­åº¦
        # NewStories ä¼šåŒ…å«å¤§é‡ 0-2 åˆ†çš„æ–°å¸–ï¼Œæ•…ç§»é™¤
        endpoints = [
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            "https://hacker-news.firebaseio.com/v0/beststories.json"
        ]
        
        candidates = []
        for ep in endpoints:
            try:
                # è·å–å‰ 50 æ¡ï¼Œç¡®ä¿è¦†ç›–é¢
                ids = session.get(ep, timeout=5).json()[:50]
                candidates.extend(ids)
            except: pass
            
        # å»é‡
        candidates = list(set(candidates))
            
        ai_stories = []
        keywords = ["AI", "GPT", "LLM", "Diffusion", "Model", "Neural", "Transformer", "Agent", "RAG", "DeepSeek", "OpenAI"]
        
        for hid in candidates[:80]: # é™åˆ¶è¯·æ±‚æ¬¡æ•°
            try:
                item = session.get(f"https://hacker-news.firebaseio.com/v0/item/{hid}.json", timeout=3).json()
                if not item or "title" not in item: continue
                
                # è¿‡æ»¤æ‰ä½çƒ­åº¦å¸–å­ (Score < 50)
                score = item.get("score", 0)
                if score < 50:
                    continue

                title = item["title"]
                if any(k.lower() in title.lower() for k in keywords):
                    ai_stories.append({
                        "title": title,
                        "url": item.get("url", f"https://news.ycombinator.com/item?id={hid}"),
                        "score": score,
                        "type": item.get("type", "story")
                    })
                    if len(ai_stories) >= 5: break
            except: continue
        return ai_stories
    except Exception as e:
        print(f"Hacker News è·å–å¤±è´¥: {e}")
        return []

def fetch_huggingface_trending():
    """
    è·å– Hugging Face Daily Papers (ä¼˜å…ˆ) å’Œ Trending Models
    ç­–ç•¥ï¼šä¼˜å…ˆå±•ç¤ºæ¯æ—¥è®ºæ–‡ (ç»å¯¹æ–°é²œ)ï¼Œè¾…ä»¥çƒ­é—¨æ¨¡å‹
    """
    print("æ­£åœ¨è·å– Hugging Face çƒ­é—¨å†…å®¹...")
    data_summary = {"papers": [], "models": []}
    
    # 1. Fetch Daily Papers (Scraping)
    try:
        url = "https://huggingface.co/papers"
        r = curl_cffi.requests.get(url, impersonate="chrome120", verify=False, timeout=10)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = soup.find_all('article')
            papers = []
            for art in articles[:5]: # Top 5 papers
                title_tag = art.find('h3')
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    link = "https://huggingface.co" + art.find('a')['href'] if art.find('a') else ""
                    # Try to find upvotes/likes
                    likes_div = art.find('div', class_='leading-none')
                    likes = likes_div.get_text(strip=True) if likes_div else "N/A"
                    
                    papers.append({
                        "title": title,
                        "url": link,
                        "likes": likes,
                        "type": "Paper"
                    })
            data_summary["papers"] = papers
    except Exception as e:
        print(f"HF Papers error: {e}")

    # 2. Fetch Trending Models (API: sort=likes7d to ensure freshness)
    # Using curl_cffi to avoid 400/403
    try:
        # sort=likes7d gets most liked in last 7 days (fresh!)
        url = "https://huggingface.co/api/models?sort=likes7d&limit=5"
        r = curl_cffi.requests.get(url, impersonate="chrome120", verify=False, timeout=10)
        if r.status_code == 200:
            models = r.json()
            model_list = []
            for m in models:
                # Filter out very old models if possible, but likes7d usually handles this.
                # We can check 'createdAt' if the API returns it, but it might not in the summary.
                model_list.append({
                    "name": m["modelId"],
                    "likes_7d": m.get("likes", 0), # In this sort, likes might be total or 7d? API isn't always clear, but the rank is correct.
                    "url": f"https://huggingface.co/{m['modelId']}",
                    "tags": m.get("tags", [])[:3]
                })
            data_summary["models"] = model_list
    except Exception as e:
        print(f"HF Models error: {e}")

    return data_summary

def fetch_rss_data(url, limit=10):
    print(f"æ­£åœ¨è·å– RSS: {url} ...")
    try:
        feed = feedparser.parse(url)
        items = []
        now = datetime.datetime.now(datetime.timezone.utc)
        
        for entry in feed.entries:
            # Calculate Time
            published_time = None
            time_label = "æœ€æ–°"
            
            try:
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_time = datetime.datetime.fromtimestamp(calendar.timegm(entry.published_parsed), datetime.timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published_time = datetime.datetime.fromtimestamp(calendar.timegm(entry.updated_parsed), datetime.timezone.utc)
                
                if published_time:
                    diff = now - published_time
                    seconds = diff.total_seconds()
                    
                    if seconds > 86400: # Skip older than 24h
                        continue
                        
                    hours = int(seconds / 3600)
                    if hours < 1:
                        time_label = "ææ–° <1å°æ—¶"
                    elif hours < 12:
                        time_label = f"ææ–° {hours}å°æ—¶å‰"
                    else:
                        time_label = "æœ€æ–° æ˜¨æ—¥"
            except Exception as e:
                pass

            items.append({
                "title": entry.title, 
                "link": entry.link, 
                "summary": entry.get("summary", ""),
                "published": entry.get("published", ""),
                "time_label": time_label
            })
            
            if len(items) >= limit:
                break
        
        return items
    except Exception as e:
        print(f"RSS error {url}: {e}")
        return []

def fetch_macro_news():
    """è·å–å®è§‚ç»æµå®æ—¶æ–°é—» (RSS)"""
    print("æ­£åœ¨è·å–å®è§‚ç»æµæ–°é—»...")
    # Investing.com Economy & General + CNBC Economy
    urls = [
        "https://www.investing.com/rss/news_14.rss", # Economy
        "https://www.cnbc.com/id/20910258/device/rss/rss.html", # CNBC Economy
        "https://feeds.content.dowjones.io/public/rss/mw_topstories" # MarketWatch Top Stories
    ]
    news_items = []
    for url in urls:
        items = fetch_rss_data(url, limit=10)
        news_items.extend(items)
    return news_items

class DailyReport:
    def __init__(self):
        self.monitor = IndicatorMonitor()
    
    def generate_report(self):
        print("ğŸš€ å¼€å§‹æ‰§è¡Œ AI æ—¥æŠ¥ç”Ÿæˆä»»åŠ¡...")
        
        # 1. Fetch All Data
        market_summary = self.monitor.run()
        github_data = fetch_github_trending()
        hn_data = fetch_hacker_news_ai()
        hf_data = fetch_huggingface_trending()
        macro_news = fetch_macro_news()
        
        # RSS for context
        rss_sources = [
            "https://www.coindesk.com/arc/outboundfeeds/rss/",
        ]
        rss_news = []
        for url in rss_sources:
            rss_news.extend(fetch_rss_data(url, limit=2))

        # 2. LLM Processing
        print("ğŸ¤– æ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆ HTML æŠ¥å‘Š...")
        
        client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
        
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´ (UTC+8) é¿å…æœåŠ¡å™¨æ—¶åŒºå·®å¼‚
        beijing_now = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(hours=8)
        today = beijing_now.strftime("%Yå¹´%mæœˆ%dæ—¥ | %A")
        
        # Construct Prompt
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI ä¸é‡‘èå¸‚åœºåˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ HTML æ ¼å¼çš„æ—¥æŠ¥ã€‚
        
        **å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹ HTML ç»“æ„å’Œæ’ç‰ˆè¦æ±‚**ï¼š
        1. **ä¸è¦ä½¿ç”¨ Markdown**ï¼Œç›´æ¥è¾“å‡ºçº¯ HTML ä»£ç  (åªåŒ…å« <body> å†…çš„ div ç»“æ„)ã€‚
        2. **å¿…é¡»ä½¿ç”¨æŒ‡å®šçš„ CSS ç±»å**ï¼Œä¸è¦ä¿®æ”¹ç±»åï¼Œå› ä¸ºæ ·å¼è¡¨å·²ç»å›ºå®šã€‚
        3. **æ‰€æœ‰æ ‡é¢˜å’Œé¡¹ç›®å¿…é¡»åŒ…å«è¶…é“¾æ¥** (å¦‚æœæœ‰ URL)ã€‚

        **HTML æ¨¡æ¿ç»“æ„ (è¯·ä¸¥æ ¼æŒ‰æ­¤å¡«å……æ•°æ®)**ï¼š

        ```html
        <div class="container">
            <h1>AI & é‡‘èå¸‚åœºæ—¥æŠ¥</h1>
            <div class="date">{today}</div>

            <!-- 1. æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§ (Market Pulse) -->
            <div class="section">
                <div class="section-title"><span class="section-icon">ğŸ“‰</span> æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§</div>
                <div class="grid-container">
                    <!-- è¯·ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆä¸€ä¸ª grid-card -->
                    <!-- ç¤ºä¾‹: BTC Price -->
                    <div class="grid-card">
                        <div class="card-label">BTC Price</div>
                        <div class="card-value">$69,388</div> <!-- æ•°å€¼ -->
                        <div class="card-sub trend-down">ğŸ“‰ -1.25%</div> <!-- trend-up(ç»¿)/trend-down(çº¢)/trend-neutral(ç°) -->
                    </div>
                    <!-- å¿…é¡»åŒ…å«ä»¥ä¸‹æŒ‡æ ‡:
                         1. BTC Price
                         2. ETH Price
                         3. BTC ETF Flow (ä½¿ç”¨ trend-up/down å’Œ emoji)
                         4. ææ…Œè´ªå©ªæŒ‡æ•° (ä½¿ç”¨ bg-red/bg-green æ ‡ç­¾)
                         5. èµ„é‡‘è´¹ç‡ (Bybit) (ä¿ç•™4-6ä½å°æ•°, å¦‚ 0.0001%, ä½¿ç”¨ bg-green/red)
                         6. æŒä»“é‡ (OI)
                         7. å·¨é²¸ (>1k BTC)
                         8. ç¨³å®šå¸å¸‚å€¼
                         9. 10å¹´æœŸç¾å€º
                         10. ç¾å…ƒæŒ‡æ•° (DXY)
                         11. é»„é‡‘ (Gold)
                         12. æ ‡æ™®500 (SPX)
                    -->
                </div>
            </div>

            <!-- 2. å®è§‚ç»æµç¡¬æ ¸å¿«è®¯ -->
            <div class="section">
                <div class="section-title"><span class="section-icon">ğŸ—ï¸</span> å®è§‚ç»æµç¡¬æ ¸å¿«è®¯</div>
                <!-- å¾ªç¯ç”Ÿæˆ macro-item -->
                <div class="macro-item">
                    <div class="macro-header">
                        <a href="[URL]" class="macro-link">ğŸ“‰ [æ ‡é¢˜]: [æ ¸å¿ƒäººç‰©/æœºæ„] [è§‚ç‚¹]</a> 
                        <span class="macro-meta">[æ—¶é—´æ ‡ç­¾]</span>
                    </div>
                    <div class="macro-body">[ä¸€å¥è¯èƒŒæ™¯æˆ–å½±å“]</div>
                </div>
            </div>

            <!-- 3. GitHub æœ¬å‘¨é»‘é©¬ -->
            <div class="section">
                <div class="section-title"><span class="section-icon">ğŸ› </span> GitHub æœ¬å‘¨é»‘é©¬</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 70%">é¡¹ç›® & æ ¸å¿ƒåˆ›æ–°</th>
                            <th style="width: 30%; text-align: right;">è¶‹åŠ¿</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- å¾ªç¯ç”Ÿæˆè¡Œ -->
                        <tr>
                            <td>
                                <a href="[URL]" class="project-name">[é¡¹ç›®åç§°]</a>
                                <div class="project-desc">[æè¿°], <b>[æ ¸å¿ƒåˆ›æ–°ç‚¹/è§£å†³äº†ä»€ä¹ˆç—›ç‚¹]</b>ã€‚</div>
                            </td>
                            <td style="text-align: right;"><span class="stars-badge">â­ï¸ [Starsæ•°]</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- 4. Hugging Face å‰æ²¿ -->
            <div class="section">
                <div class="section-title"><span class="section-icon">ğŸ¤—</span> Hugging Face å‰æ²¿</div>
                <ul class="hf-list">
                    <!-- å¾ªç¯ç”Ÿæˆ hf-item -->
                    <li class="hf-item">
                        <div class="hf-header">
                            <span class="hf-tag tag-paper">Paper</span> <!-- æˆ– tag-model -->
                            <div class="hf-title"><a href="[URL]" class="hf-link">[æ ‡é¢˜]</a></div>
                        </div>
                        <div class="hf-comment">â€œ[ä¸€å¥è¯é”è¯„ï¼šæ¯”ä¹‹å‰çš„å¼ºåœ¨å“ªé‡Œï¼Ÿ]â€</div>
                    </li>
                </ul>
            </div>

            <!-- 5. AI ç¤¾åŒºçƒ­è®® -->
            <div class="section">
                <div class="section-title"><span class="section-icon">ğŸŒ</span> AI ç¤¾åŒºçƒ­è®®</div>
                <ul class="hn-list">
                    <!-- å¾ªç¯ç”Ÿæˆ hn-item -->
                    <li class="hn-item">
                        <span class="hn-bullet">ğŸ’¬</span>
                        <div class="hn-content">
                            <a href="[URL]" class="hn-link">â€œ[æ ‡é¢˜/æ ¸å¿ƒè§‚ç‚¹]â€</a> â€” [çƒ­åº¦/æ¥æº], [ä¸»è¦äº‰è®ºç‚¹]ã€‚
                        </div>
                    </li>
                </ul>
            </div>

            <!-- 6. ä»Šæ—¥æ€»ç»“ -->
            <div class="section">
                <div class="summary-box">
                    <div class="summary-title">ğŸ’¡ ä»Šæ—¥æ€»ç»“</div>
                    <div class="summary-item">1. <b>[å…³é”®è¯]</b>ï¼š[æ€»ç»“1]</div>
                    <div class="summary-item">2. <b>[å…³é”®è¯]</b>ï¼š[æ€»ç»“2]</div>
                    <div class="summary-item">3. <b>[å…³é”®è¯]</b>ï¼š[æ€»ç»“3]</div>
                </div>
            </div>
        </div>
        ```

        **æ•°æ®å¤„ç†é€»è¾‘**ï¼š
        1. **Market Pulse**: 
           - æ¶¨è·Œå¹…é¢œè‰²ï¼š`trend-up` (ç»¿è‰²/æ¶¨), `trend-down` (çº¢è‰²/è·Œ), `trend-neutral` (ç°è‰²)ã€‚
           - **æ³¨æ„**ï¼šå¯¹äºä»·æ ¼(BTC/ETH/SPX)ï¼Œæ¶¨æ˜¯å¥½äº‹(trend-up ç»¿è‰²)ï¼›å¯¹äºææ…ŒæŒ‡æ•°ï¼Œæåº¦ææ…Œæ˜¯çº¢è‰²(bg-red)ã€‚
           - èµ„é‡‘è´¹ç‡ï¼šå¦‚æœç»å¯¹å€¼ < 0.001%ï¼Œä¿ç•™ 4-6 ä½å°æ•°ï¼Œä¸è¦æ˜¾ç¤ºä¸º 0.00%ã€‚
        2. **Macro News**: ç­›é€‰ 3-5 æ¡é«˜ä»·å€¼æ–°é—»ï¼Œå¿…é¡»åŒ…å«å…·ä½“äººç‰©æˆ–æœºæ„ã€‚æ—¶é—´æ ‡ç­¾æ ¼å¼å¦‚ "ææ–° 3å°æ—¶å‰" æˆ– "æ˜¨æ—¥"ã€‚
        3. **GitHub**: é‡ç‚¹å…³æ³¨ "æœ¬å‘¨é»‘é©¬"ï¼Œå¿…é¡»ç”¨ç²—ä½“ `<b>` æ ‡å‡ºæ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚
        4. **Hugging Face**: åŒºåˆ† Paper å’Œ Modelï¼Œå¿…é¡»æœ‰ä¸€å¥è¯é”è¯„ã€‚
        5. **URL**: æ‰€æœ‰ `<a href="...">` å¿…é¡»å¡«å†™çœŸå® URLï¼Œå¦‚æœæ²¡æœ‰åˆ™å¡« `#`ã€‚

        **è¾“å…¥æ•°æ®**ï¼š
        Market Data: {json.dumps(market_summary, indent=2)}
        GitHub: {json.dumps(github_data, indent=2)}
        Hacker News: {json.dumps(hn_data, indent=2)}
        Hugging Face: {json.dumps(hf_data, indent=2)}
        Macro News (RSS): {json.dumps(macro_news, indent=2)}
        
        è¯·ç›´æ¥è¾“å‡ºç”Ÿæˆçš„ HTML ä»£ç ã€‚
        """
        
        try:
            response = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": "è¯·ç”Ÿæˆä»Šå¤©çš„æ—¥æŠ¥ HTMLã€‚"}
                ],
                temperature=0.3
            )
            html_content = response.choices[0].message.content
            # Strip markdown code blocks if present
            html_content = html_content.replace("```html", "").replace("```", "")
            
            return html_content
        except Exception as e:
            print(f"LLM ç”Ÿæˆå¤±è´¥: {e}")
            return "<h3>æŠ¥å‘Šç”Ÿæˆå¤±è´¥</h3>"

    def send_email(self, html_content):
        sender = MAIL_USER
        
        # ä¸»æ”¶ä»¶äºº (Env)
        main_receiver = MAIL_RECEIVER
        # æŠ„é€æ”¶ä»¶äºº (CC)
        cc_receivers = ["tangjx1004@163.com", "251400187@qq.com", "120750300@qq.com"]
        
        # æ‰€æœ‰æ”¶ä»¶äºº (SMTP ä¼ è¾“éœ€è¦)
        all_receivers = [main_receiver] + cc_receivers
        
        msg = MIMEMultipart('related')
        
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´
        beijing_now = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(hours=8)
        msg['Subject'] = f"ğŸ¤– AI & é‡‘èå‰æ²¿æ—¥æŠ¥ ({beijing_now.date()})"
        msg['From'] = sender
        msg['To'] = main_receiver
        msg['Cc'] = ",".join(cc_receivers)
        
        # CSS from preview_layout_v2.html
        css_styles = """
    /* å…¨å±€é‡ç½®ä¸å­—ä½“ */
    body { 
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; 
        line-height: 1.6; 
        color: #37352f; 
        background-color: #f7f7f5;
        margin: 0; 
        padding: 40px 20px; 
    }

    .container { 
        max-width: 800px; 
        margin: 0 auto; 
        background: #ffffff; 
        padding: 60px; 
        min-height: 100vh;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05); 
    }

    h1 { font-size: 32px; margin: 0 0 8px 0; font-weight: 700; letter-spacing: -0.5px; }
    .date { color: #787774; font-size: 14px; margin-bottom: 48px; border-bottom: 1px solid #e0e0e0; padding-bottom: 24px; }
    
    .section { margin-bottom: 48px; }
    .section-title { 
        font-size: 20px; font-weight: 600; margin-bottom: 16px; 
        display: flex; align-items: center; color: #37352f; 
        padding-bottom: 8px; border-bottom: 2px solid #f1f1ef;
    }
    .section-icon { margin-right: 8px; font-size: 24px; }

    /* é€šç”¨é“¾æ¥æ ·å¼ */
    a { text-decoration: none; color: inherit; transition: color 0.2s; }
    a:hover { color: #2563eb; text-decoration: underline; }

    /* --- 1. æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§ (Market Pulse) - ç´§å‡‘ç½‘æ ¼å¸ƒå±€ --- */
    .grid-container {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
        gap: 12px;
    }

    .grid-card {
        background: #f7f9fb;
        border: 1px solid #eef0f2;
        border-radius: 6px;
        padding: 12px;
        display: flex;
        flex-direction: column;
        justify-content: center;
        transition: all 0.2s;
    }
    .grid-card:hover { border-color: #d1d5db; transform: translateY(-1px); }
    
    .card-label { 
        font-size: 12px; color: #6b7280; margin-bottom: 4px; font-weight: 500;
        white-space: nowrap; overflow: hidden; text-overflow: ellipsis;
    }
    .card-value { 
        font-size: 18px; font-weight: 700; color: #111827; 
        font-family: -apple-system, BlinkMacSystemFont, monospace; letter-spacing: -0.5px;
    }
    .card-sub { font-size: 11px; margin-top: 4px; font-weight: 500; display: flex; align-items: center; }
    
    .trend-up { color: #059669; }
    .trend-down { color: #dc2626; }
    .trend-neutral { color: #6b7280; }
    .bg-green { background-color: #ecfdf5; padding: 2px 6px; border-radius: 4px; color: #059669; }
    .bg-red { background-color: #fef2f2; padding: 2px 6px; border-radius: 4px; color: #dc2626; }

    /* --- 2. å®è§‚ç¡¬æ ¸å¿«è®¯ (Macro) - å‚ç›´åˆ—è¡¨ --- */
    .macro-item { margin-bottom: 20px; padding-left: 16px; border-left: 3px solid #3b82f6; }
    .macro-header { font-size: 15px; font-weight: 600; color: #1f2937; margin-bottom: 4px; }
    .macro-link { color: #1f2937; text-decoration: none; }
    .macro-link:hover { color: #2563eb; text-decoration: underline; }
    .macro-meta { font-size: 11px; color: #d97706; background: #fffbeb; padding: 2px 6px; border-radius: 4px; margin-left: 8px; vertical-align: middle; display: inline-block;}
    .macro-body { font-size: 14px; color: #4b5563; line-height: 1.5; }

    /* --- 3. GitHub Trending - æ¸…çˆ½å®½è¡¨æ ¼ --- */
    table { width: 100%; border-collapse: collapse; font-size: 14px; }
    th { text-align: left; border-bottom: 1px solid #e5e7eb; padding: 12px 8px; color: #9ca3af; font-size: 12px; text-transform: uppercase; letter-spacing: 0.5px; }
    td { border-bottom: 1px solid #f3f4f6; padding: 16px 8px; vertical-align: top; }
    .project-name { font-weight: 600; color: #2563eb; font-size: 15px; margin-bottom: 4px; display: block; text-decoration: none; }
    .project-desc { color: #374151; font-size: 14px; line-height: 1.5; }
    .stars-badge { font-family: monospace; color: #059669; background: #ecfdf5; padding: 4px 8px; border-radius: 6px; font-size: 12px; font-weight: 600; white-space: nowrap; }

    /* --- 4. Hugging Face - å‚ç›´å¡ç‰‡æµ --- */
    .hf-list { list-style: none; padding: 0; margin: 0; }
    .hf-item { margin-bottom: 20px; padding-bottom: 20px; border-bottom: 1px solid #f3f4f6; }
    .hf-item:last-child { border-bottom: none; }
    .hf-header { display: flex; align-items: baseline; margin-bottom: 6px; }
    .hf-tag { font-size: 10px; padding: 2px 6px; border-radius: 4px; margin-right: 8px; font-weight: 700; text-transform: uppercase; }
    .tag-paper { background: #eff6ff; color: #2563eb; }
    .tag-model { background: #fff7ed; color: #c2410c; }
    .hf-title { font-weight: 600; font-size: 16px; }
    .hf-link { color: #111827; text-decoration: none; }
    .hf-link:hover { color: #2563eb; text-decoration: underline; }
    .hf-comment { font-size: 14px; color: #4b5563; background: #f9fafb; padding: 10px; border-radius: 6px; border-left: 3px solid #9ca3af; margin-top: 8px; }

    /* --- 5. AI çƒ­è®® (Hacker News) --- */
    .hn-list { list-style: none; padding: 0; margin: 0; }
    .hn-item { margin-bottom: 16px; font-size: 14px; line-height: 1.5; color: #374151; display: flex; align-items: flex-start; }
    .hn-bullet { color: #f59e0b; margin-right: 10px; font-size: 16px; line-height: 1.5; }
    .hn-link { color: #111827; font-weight: 700; text-decoration: none; }
    .hn-link:hover { color: #2563eb; text-decoration: underline; }

    /* --- 6. æ€»ç»“åŒº --- */
    .summary-box { background: #fffbeb; border: 1px solid #fcd34d; border-radius: 8px; padding: 24px; }
    .summary-title { font-weight: 700; margin-bottom: 16px; color: #92400e; display: flex; align-items: center; }
    .summary-item { margin-bottom: 12px; font-size: 15px; line-height: 1.6; color: #78350f; }
    .summary-item b { color: #451a03; }
        """
        
        full_html = f"""
        <html>
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <style>
            {css_styles}
            </style>
        </head>
        <body>
            {html_content}
            <div style="margin-top: 40px; font-size: 12px; color: #888; text-align: center;">
                Generated by Trae AI Assistant â€¢ {datetime.datetime.now().strftime("%Y-%m-%d %H:%M")}
            </div>
        </body>
        </html>
        """
        
        msg.attach(MIMEText(full_html, 'html', 'utf-8'))
        
        try:
            server = smtplib.SMTP_SSL(MAIL_HOST, MAIL_PORT)
            server.login(MAIL_USER, MAIL_PASS)
            server.sendmail(sender, all_receivers, msg.as_string())
            server.quit()
            print("âœ… é‚®ä»¶å‘é€æˆåŠŸ (å«æŠ„é€)ï¼")
        except Exception as e:
            print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")

if __name__ == "__main__":
    report = DailyReport()
    html = report.generate_report()
    if html:
        report.send_email(html)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
